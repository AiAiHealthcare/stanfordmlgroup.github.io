<!DOCTYPE html><!-- Author: Pranav Rajpurkar 2017--><html><head><meta charset="utf-8"><title>Deep learning for chest radiograph diagnosis:A retrospective comparison of the CheXNeXt algorithm to practicing radiologists</title><meta name="description" content="Detecting Diseases from Chest X-Rays at the level of Radiologists."><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><meta property="og:image" content="/logo.jpg"><link rel="image_src" type="image/jpeg" href="/logo.jpg"><link rel="shortcut icon" href="/favicon.ico" type="image/x-icon"><link rel="icon" href="/favicon.ico" type="image/x-icon"><link href="/lib/bootstrap/css/bootstrap.min.css" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Lato:400,600" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Muli:400,600" rel="stylesheet"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="/lib/simple-line-icons/css/simple-line-icons.css"><link href="/css/theme.css" rel="stylesheet"><link rel="stylesheet" type="text/css" href="/projects/chexnext/css/chexnext.css"><script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script><script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script><script src="/js/analytics.js"></script></head><body><nav class="navbar navbar-default navbar-fixed-top" id="mainNav"><div class="container"><!-- Brand and toggle get grouped for better mobile display--><div class="navbar-header"><a class="navbar-brand page-scroll" href="/">Stanford ML Group</a></div><!-- Collect the nav links, forms, and other content for toggling--></div></nav><section id="header"><div class="container"><div class="row"><div class="col-lg-8"><h1 id="page-title">CheXNeXt: Deep learning for chest radiograph diagnosis</h1><h3>Pranav Rajpurkar*, Jeremy Irvin*, Robyn L. Ball, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti Bagul, Curtis P. Langlotz, Bhavik N. Patel, Kristen W. Yeom, Katie Shpanskaya, Francis G. Blankenberg, Jayne Seekins, Timothy J. Amrhein, David A. Mong, Safwan S. Halabi, Evan J. Zucker, Andrew Y. Ng, Matthew P. Lungren</h3></div></div></div></section><section><div class="container"><div class="row"><div class="col-md-6 col-md-push-6"><h3>We developed CheXNeXt, a convolutional neural network to concurrently detect the presence of multiple pathologies, including pneumonia, pleural effusion, pulmonary masses, and nodules in frontal-view chest radiographs.</h3><p>Chest radiograph interpretation is critical for the detection of acute thoracic diseases, including tuberculosis and lung cancer, which affect millions of people worldwide each year. This time-consuming task typically requires expert radiologists to read the images, leading to fatigue-based diagnostic error and lack of diagnostic expertise in areas of the world where radiologists are not available.</p></div><div class="col-sm-12 col-md-6 col-md-pull-6"><iframe width="100%" height="400px" src="https://www.youtube.com/embed/VJRCj-4E2iU?ecver=1" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div></div></div></section><section class="gray"><div class="container"><div class="row"><div class="col-md-7"><img src="/projects/chexnet/img/chex-main.svg" style="max-height:700px;"></div><div class="col-md-5"><h2>Our model, CheXNet, is a 121-layer convolutional neural network that inputs a chest X-ray image and outputs the probability of pneumonia along with a heatmap localizing the areas of the image most indicative of pneumonia. </h2><p>We train CheXNet on the recently released ChestX-ray14 dataset, which contains 112,120 frontal-view chest X-ray images individually labeled with up to 14 different thoracic diseases, including pneumonia. We use dense connections and batch normalization to make the optimization of such a deep network tractable.</p></div></div></div></section><section><div class="container"><div class="row"><div class="col-md-7"><h2>We train on ChestX-ray14, the largest publicly available chest X- ray dataset.</h2><p>The dataset, released by the NIH, contains 112,120 frontal-view X-ray images of 30,805 unique patients, annotated with up to 14 different thoracic pathology labels using NLP methods on radiology reports. We label images that have pneumonia as one of the annotated pathologies as positive examples and label all other images as negative examples for the pneumonia detection task.</p><p>We collected a test set of 420 frontal chest X-rays. Annotations were obtained independently from four practicing radiologists at Stanford University, who were asked to label all 14 pathologies, even though . We then evaluate the performance of an individual radiologist by using the majority vote of the other 3 radiologists as ground truth. Similarly, we evaluate CheXNet using the majority vote of 3 of 4 radiologists, repeated four times to cover all groups of 3.</p></div><div class="col-md-5"><img src="/projects/chexnet/img/chest-example2.png"></div></div></div></section><section><div class="container"><div class="row"><div class="col-md-7"><img src="/projects/chexnet/img/results.png"></div><div class="col-md-5"><h2>We find that the model exceeds the average radiologist performance on the pneumonia detection task.</h2><p> We compute the F1 score for each individual radiologist
and for CheXNet against each of the other 4 labels
as ground truth. We report
the mean of the 4 resulting F1 scores for each radiologist
and for CheXNet, along with the average F1
across the radiologists.
We compare radiologists and our model on the F1
metric, which is the harmonic average of the precision and
recall. CheXNet achieves an F1 score of 0.435
(95% CI 0.387, 0.481), higher than the radiologist average
of 0.387 (95% CI 0.330, 0.442). We use the bootstrap to
find that the difference in performance is statistically significant.</p></div></div></div></section><section class="gray"><div class="container"><div class="row"><div class="col-md-7"><h3> With approximately 2 billion procedures per year, chest X-rays are the most common imaging examination tool used in practice, critical for screening, diagnosis, and management of diseases including pneumonia. However, an estimated two thirds of the global population lacks access to radiology diagnostics. With automation at the level of experts, we hope that this technology can improve healthcare delivery and increase access to medical imaging expertise in parts of the world where access to skilled radiologists is limited.</h3><a class="btn btn-lg btn-default" href="https://arxiv.org/abs/1711.05225">Read our paper</a></div></div></div></section><section class="bg-primary"><div class="container"><div class="row"><div class="col-md-12"><h3>If you have questions about our work,
contact us at:</h3><h4><code>pranavsr@cs.stanford.edu</code> and <code>jirvin16@cs.stanford.edu</code></h4></div></div></div></section><footer><div class="container"><div class="row"><div class="col-md-12 text-center"><a href="/"><img src="/img/stanfordmlgrouplogo.svg"></a></div></div></div></footer><script src="/lib/jquery/jquery.min.js"></script><script src="/lib/bootstrap/js/bootstrap.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script><script src="/js/theme.js"></script></body></html>