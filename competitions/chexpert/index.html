<!DOCTYPE html><!-- Author: Pranav Rajpurkar 2017--><html><head><meta charset="utf-8"><title>CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison</title><meta name="description" content="CheXpert is a large dataset of chest X-rays. Algorithms are tasked with determining whether an X-ray contains up to five pathologies."><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><link rel="image_src" type="image/jpeg" href="/logo.jpg"><link rel="shortcut icon" href="/favicon.ico" type="image/x-icon"><link rel="icon" href="/favicon.ico" type="image/x-icon"><link href="/lib/bootstrap/css/bootstrap.min.css" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Lato:400,600" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Muli:400,600" rel="stylesheet"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="/lib/simple-line-icons/css/simple-line-icons.css"><link href="/css/theme.css" rel="stylesheet"><link href="//cdn-images.mailchimp.com/embedcode/classic-10_7.css" rel="stylesheet" type="text/css"><link rel="stylesheet" type="text/css" href="/competitions/chexpert/css/index.css"><script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script><script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script><script src="/js/analytics.js"></script></head><body><nav class="navbar navbar-default navbar-fixed-top" id="mainNav"><div class="container"><!-- Brand and toggle get grouped for better mobile display--><div class="navbar-header"><a class="navbar-brand page-scroll" href="/">Stanford ML Group</a></div><!-- Collect the nav links, forms, and other content for toggling--></div></nav><section id="header"><div class="container"><div class="row"><div class="col-lg-12"><img id="title-image" src="/competitions/chexpert/img/logo.svg"><h2 id="page-subtitle">Chest X-Ray Deep Learning Competition</h2></div></div></div></section><section><div class="container"><div class="row"><div class="col-md-6"><h1>What is CheXpert?</h1><p>CheXpert is a large dataset of chest X-rays. Algorithms are tasked with determining whether an X-ray contains up to five pathologies.</p><p>Chest radiography is the most common imaging examination globally, critical for screening, diagnosis, and management of many life threatening diseases. Automated chest radiograph interpretation at the level of practicing radiologists could provide substantial benefit in many medical settings, from improved workflow prioritization and clinical decision support to large-scale screening and global population health initiatives.</p><p>CheXpert is one of the largest public radiographic image datasets. We're making this dataset available to the community and hosting a competition to see if your models can perform as well as radiologists on the task.</p><h2>How can I participate?</h2><p>CheXpert uses a hidden test set for official evaluation of models. Teams submit their executable code on Codalab, which is then run on a test set that is not publicly readable. Such a setup preserves the integrity of the test results.</p><p>Here's a tutorial walking you through official evaluation of your model. Once your model has been evaluated officially, your scores will be added to the leaderboard.</p><ul class="list-inline"><li><a class="btn btn-lg btn-default" href="https://worksheets.codalab.org/worksheets/0x693b0063ee504702b21f94ffb2d99c6d/">Submission Tutorial</a></li><li><a class="btn btn-lg btn-default" href="https://arxiv.org/abs/FIXME">Read the Paper</a></li></ul></div><div class="col-md-6"><h1>Leaderboard</h1><p> Will your model perform as well as radiologists in detecting pathology in chest X-rays?</p><table class="table performanceTable"><tr><th>Rank</th><th>Date</th><th>Model</th><th>AUC</th></tr><tr class="human-row"><td></td><td></td><td>Best Radiologist Performance <em>Stanford University </em><a href="https://arxiv.org/abs/FIXME">Irvin & Rajpurkar et al., 19</a></td><td>0.778</td></tr><tr><td class="rank">1 <br></td><td><span class="date label label-default">May 23, 2018</span></td><td style="word-break:break-word;">Stanford Baseline (ensemble)<em> Stanford University </em><a class="link" href="https://arxiv.org/abs/1712.06957">https://arxiv.org/abs/1712.06957</a></td><td><b>0.705</b></td></tr></table></div></div></div></section><section class="gray"><div class="container"><div class="row"><div class="col-md-7"><h2>How did we collect and label CheXpert?</h2><p>CheXpert is a large public dataset for chest radiograph interpretation, consisting of 224,316 chest radiographs of 65,240 patients. We retrospectively collected the chest radiographic examinations from Stanford Hospital, performed between October 2002 and July 2017 in both inpatient and outpatient centers, along with their associated radiology reports.</p><h3>Label Extraction from Radiology Reports</h3><p>Each report was labeled for the presence of 14 observations as positive, negative, or uncertain. We decided on the 14 observations based on the prevalence in the reports and clinical relevance, conforming to <a href="https://pubs.rsna.org/doi/10.1148/radiol.2462070712">the Fleischner Societyâ€™s recommended glossary </a>whenever applicable. We then developed an automated rule-based labeler to extract observations from the free text radiology reports to be used as structured labels for the images.</p><p>Our labeler is set up in three distinct stages: mention extraction, mention classification, and mention aggregation. In the mention extraction stage, the labeler extracts mentions from a list of observations from the <i>Impression</i> section of radiology reports, which summarizes the key findings in the radiographic study. In the mention classification stage, mentions of observations are classified as negative, uncertain, or positive. In the mention aggregation stage, we use the classification for each mention of observations to arrive at a final label for the 14 observations.</p><a class="btn btn-lg btn-default" href="https://github.com/stanfordmlgroup/chexpert-labeler">Use the labeling tool</a></div><div class="col-md-5"><img src="/competitions/chexpert/img/table1.png"><img src="/competitions/chexpert/img/figure2.png"></div></div></div></section><section><div class="container"><div class="row"><div class="col-md-7"><h2>What is our baseline model?</h2><p>We train models that take as input a single-view chest radiograph and output the probability of each of the 14 observations. When more than one view is available, the models output the maximum probability of the observations across the views.</p><h3>Leveraging Uncertainty Labels</h3><p>We explore different approaches to using the uncertainty labels during the model training.</p><p><b>U-Ignore</b>: We ignore the uncertain labels during training.</p><p><b>U-Zeroes</b>: We map all instances of the uncertain label to 0.</p><p><b>U-Ones</b>: We map all instances of the uncertain label to 1.</p><p><b>U-SelfTrained</b>: We first train a model using the U-Ignore approach to convergence, and then use the model to make predictions that re-label each of the uncertainty labels with the probability prediction outputted by the model.</p><p><b>U-MultiClass</b>: We treat the uncertainty label as its own class.</p></div><div class="col-md-5"><img src="/competitions/chexpert/img/figure1.png"></div></div></div></section><section class="gray"><div class="container"><div class="row"><div class="col-md-12"><h2>How do the modeling approaches compare?</h2><p>We compare the performance of the different uncertainty approaches on a validation set of 200 studies on which the consensus of three radiologist annotations serves as ground truth. We evaluate the approaches using the area under the receiver operating characteristic curve (AUC) metric. We focus on the evaluation of 5 observations which we call the <i>competition tasks</i>, selected based of clinical importance and prevalence in the validation set.</p><img src="/competitions/chexpert/img/table3.png"></div></div></div></section><section><div class="container"><div class="row"><div class="col-md-6"><h2>How does our baseline do?</h2><p>We compare the performance of our final model to radiologists on a test set of 500 studies on which the consensus of five radiologist annotations serves as ground truth. Three separate radiologist annotations were used to benchmark radiologist performance. We selected the final model based on the best performing ensemble on each competition task on the validation set: <i>U-Ones</i> for Atelectasis and Edema, <i>U-MultiClass</i> for Cardiomegaly and Pleural Effusion, and <i>U-SelfTrained</i> for Consolidation. We visualize the areas of the radiograph which the model predicts to be most indicative of each observation using Gradient-weighted Class Activation Mappings.</p></div><div class="col-md-offset-1 col-md-4"><img src="/competitions/chexpert/img/figure4a.png"></div><div class="col-md-12"><img src="/competitions/chexpert/img/figure3.png"></div></div></div></section><section class="gray"><div class="container"><div class="row"><div class="col-md-7"><h1>Downloading the Dataset (v1.0)</h1><p>Please read the Stanford University School of Medicine CheXpert Dataset Research Use Agreement. Once you register to download the CheXpert dataset, you will receive a link to the download over email. Note that you may not share the link to download the dataset with others.</p><div class="well" id="agreement"><h3>Stanford University School of Medicine CheXpert Dataset Research Use Agreement</h3>
<p>By registering for downloads from the CheXpert Dataset, you are agreeing to this Research Use Agreement, as well as to the Terms of Use of the Stanford University School of Medicine website as posted and updated periodically at http://www.stanford.edu/site/terms/.</p>
<p>1. Permission is granted to view and use the CheXpert Dataset without charge for personal, non-commercial research purposes only. Any commercial use, sale, or other monetization is prohibited.</p>
<p>2. Other than the rights granted herein, the Stanford University School of Medicine (&ldquo;School of Medicine&rdquo;) retains all rights, title, and interest in the CheXpert Dataset.</p>
<p>3. You may make a verbatim copy of the CheXpert Dataset for personal, non-commercial research use as permitted in this Research Use Agreement. If another user within your organization wishes to use the CheXpert Dataset, they must register as an individual user and comply with all the terms of this Research Use Agreement.</p>
<p>4. YOU MAY NOT DISTRIBUTE, PUBLISH, OR REPRODUCE A COPY of any portion or all of the CheXpert Dataset to others without specific prior written permission from the School of Medicine.</p>
<p>5. YOU MAY NOT SHARE THE DOWNLOAD LINK to the CheXpert dataset to others. If another user within your organization wishes to use the CheXpert Dataset, they must register as an individual user and comply with all the terms of this Research Use Agreement.</p>
<p>6. You must not modify, reverse engineer, decompile, or create derivative works from the CheXpert Dataset. You must not remove or alter any copyright or other proprietary notices in the CheXpert Dataset.</p>
<p>7. The CheXpert Dataset has not been reviewed or approved by the Food and Drug Administration, and is for non-clinical, Research Use Only. In no event shall data or images generated through the use of the CheXpert Dataset be used or relied upon in the diagnosis or provision of patient care.</p>
<p>8. THE CheXpert DATASET IS PROVIDED "AS IS," AND STANFORD UNIVERSITY AND ITS COLLABORATORS DO NOT MAKE ANY WARRANTY, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE, NOR DO THEY ASSUME ANY LIABILITY OR RESPONSIBILITY FOR THE USE OF THIS CheXpert DATASET.</p>
<p>9. You will not make any attempt to re-identify any of the individual data subjects. Re-identification of individuals is strictly prohibited. Any re-identification of any individual data subject shall be immediately reported to the School of Medicine.</p>
<p>10. Any violation of this Research Use Agreement or other impermissible use shall be grounds for immediate termination of use of this CheXpert Dataset. In the event that the School of Medicine determines that the recipient has violated this Research Use Agreement or other impermissible use has been made, the School of Medicine may direct that the undersigned data recipient immediately return all copies of the CheXpert Dataset and retain no copies thereof even if you did not cause the violation or impermissible use.</p>
<p>In consideration for your agreement to the terms and conditions contained here, Stanford grants you permission to view and use the CheXpert Dataset for personal, non-commercial research. You may not otherwise copy, reproduce, retransmit, distribute, publish, commercially exploit or otherwise transfer any material.</p>
<h4>Limitation of Use</h4>
<p>You may use CheXpert Dataset for legal purposes only.</p>
<p>You agree to indemnify and hold Stanford harmless from any claims, losses or damages, including legal fees, arising out of or resulting from your use of the CheXpert Dataset or your violation or role in violation of these Terms. You agree to fully cooperate in Stanford&rsquo;s defense against any such claims. These Terms shall be governed by and interpreted in accordance with the laws of California. </p></div><!-- Begin MailChimp Signup Form -->
<div id="mc_embed_signup">
<form action="https://google.us13.list-manage.com/subscribe/post?u=1842e6560d6e10316b4e1aaf5&amp;id=6eef59b9d5" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
<div id="mc_embed_signup_scroll">
<!-- <h2>Register To Download CheXpert</h2> -->
<div class="mc-field-group">
<label for="mce-EMAIL">Email Address </label>
<input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL">
</div>
<div class="mc-field-group">
<label for="mce-FNAME">First Name </label>
<input type="text" value="" name="FNAME" class="required" id="mce-FNAME">
</div>
<div class="mc-field-group">
<label for="mce-LNAME">Last Name </label>
<input type="text" value="" name="LNAME" class="required" id="mce-LNAME">
</div>
<div class="mc-field-group size1of2">
<label for="mce-PHONE">Phone Number </label>
<input type="text" name="PHONE" class="required" value="" id="mce-PHONE">
</div>
<div class="mc-field-group">
<label for="mce-ORG">School / Organization </label>
<input type="text" value="" name="ORG" class="required" id="mce-ORG">
</div>
<div class="mc-field-group">
<label for="mce-TITLE">Role / Title </label>
<input type="text" value="" name="TITLE" class="required" id="mce-TITLE">
</div>
<div id="mce-responses" class="clear">
<div class="response" id="mce-error-response" style="display:none"></div>
<div class="response" id="mce-success-response" style="display:none"></div>
</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
<div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_1842e6560d6e10316b4e1aaf5_6eef59b9d5" tabindex="-1" value=""></div>
<div class="clear"><input type="submit" value="Register" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
</div>
</form>
</div>
<script type='text/javascript' src='//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js'></script><script type='text/javascript'>(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';fnames[4]='PHONE';ftypes[4]='phone';fnames[7]='ORG';ftypes[7]='text';fnames[3]='TITLE';ftypes[3]='text';}(jQuery));var $mcj = jQuery.noConflict(true);</script>
<!--End mc_embed_signup--></div></div></div></section><section><div class="container"><div class="row"><div class="col-md-7"><h2>CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison.</h2><h3>Jeremy Irvin*, Pranav Rajpurkar*, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, Jayne Seekins, David A. Mong, Safwan S. Halabi, Jesse K. Sandberg, Ricky Jones, David B. Larson, Curtis P. Langlotz, Bhavik N. Patel, Matthew P. Lungren, Andrew Y. Ng</h3><p>If you have questions about our work,
contact us at our <a href="https://groups.google.com/forum/#!forum/FIXME">google group</a>.</p><a class="btn btn-lg btn-default" href="https://arxiv.org/abs/FIXME">Read the Paper</a></div></div></div></section><footer><div class="container"><div class="row"><div class="col-md-12 text-center"><a href="/"><img src="/img/stanfordmlgrouplogo.svg"></a></div></div></div></footer><script src="/lib/jquery/jquery.min.js"></script><script src="/lib/bootstrap/js/bootstrap.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script><script src="/js/theme.js"></script><script src="/competitions/chexpert/js/form.js"></script></body></html>